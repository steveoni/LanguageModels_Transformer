## Language Model (Rough work)

`NOT WELL DOCUMENTED`

Using Transformer to build Language Model. Still work in progress. 

The maing goal is to implement something similar to GPT2. but for now just ussing the same archticture in the original Transformer model for language modeling.

The `GP.ipynb` , I just remove the encoder and re-write some part of the code from [havard nlp](https://nlp.seas.harvard.edu/2018/04/03/attention.html) code.

`GPT.ipynb`, add the gpt-2 features apart from the BPE tokenization. 

`GPT2.ipynb` add the BPE tokenization using huggingface tokenizer 



